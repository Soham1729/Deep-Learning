{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy,re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embedding\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        # Call the constructor of the parent class (nn.Embedding) using super()\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional Embedding\n",
    "class Pos_embedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, embed_dim):\n",
    "        \n",
    "        super(Pos_embedding, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        pe_mat = torch.zeros(max_seq_len, embed_dim)\n",
    "        for i in range(max_seq_len):\n",
    "            for j in range(0,embed_dim,2):\n",
    "                pe_mat[i,j] = math.sin(i / (10000 ** ((2 * j)/self.embed_dim)))\n",
    "                pe_mat[i,j+1] = math.cos(i/ (10000** ((2*j)/embed_dim)))\n",
    "        pe_mat = pe_mat.unsqueeze(0)\n",
    "        self.register_buffer('pe_mat', pe_mat)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe_mat[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "               \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multihead_attention(nn.Module):\n",
    "    def __init__(self, embed_dim = 512, n_heads = 8):\n",
    "        \n",
    "        super(Multihead_attention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.singlehead_dim = int(self.embed_dim/self.n_heads)\n",
    "        \n",
    "        self.query = nn.Linear(self.singlehead_dim, self.singlehead_dim, bias = False)\n",
    "        self.key = nn.Linear(self.singlehead_dim, self.singlehead_dim, bias = False)\n",
    "        self.value = nn.Linear(self.singlehead_dim, self.singlehead_dim, bias = False)\n",
    "        self.out = nn.Linear(self.singlehead_dim*self.n_heads, self.embed_dim, bias = False)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "145272517bddf6c1595b5c05b017fa375f010a8c8523682c34f84359a8aa7b70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
